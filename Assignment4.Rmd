---
title: "Assignment 4 - Applying meta-analytic priors"
author: "Riccardo Fusaroli"
date: "3/16/2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(readxl);library(brms);library(ggplot2);library(tibble);library(brmstools);library(data.table);library(rethinking)
library(bayesplot);library(cowplot)


meta_data = read_excel("Assignment4MetaData.xlsx")
pitch_data = read_excel("Assignment4PitchDatav2.xlsx")

```

## Assignment 4

In this assignment we do the following:
- we reproduce the meta-analysis of pitch SD from last semester in a Bayesian framework
- we reproduce the pitch SD in schizophrenia analysis from last semester using both a conservative and a meta-analytic prior
- we assess the difference in model quality and estimates using the two priors.



The questions you need to answer are: What are the consequences of using a meta-analytic prior? Evaluate the models with conservative and meta-analytic priors. Discuss the effects on estimates. Discuss the effects on model quality. Discuss the role that meta-analytic priors should have in scientific practice (- is there an disadvantage in conservative priors. is there a problem in replacing conservative priors with metanalytic priors - a few lines are enough). Should we systematically use them? Do they have drawbacks? 

Should we use them to complement more conservative approaches? How does the use of meta-analytic priors you suggest reflect the skeptical and cumulative nature of science?

### Step by step suggestions

Step 1: Reproduce the meta-analysis of pitch sd from previous studies of voice in schizophrenia
- the data is available as Assignment4MetaData.xlsx
- Effect size (cohen's d), sd and variance are already calculated (you're welcome!)
- Since we're only interested in getting a meta-analytic effect size, let's take a shortcut and use bromance magic (brms): https://mvuorre.github.io/post/2016/2016-09-29-bayesian-meta-analysis/    <- how to do last years meta analysis both using the bromance package instead 

#own notes 
regression model that estimates the overall effect size across all studies 

```{r}

#MeanES = mean effect size
#varianceES
#sdES


meta_data$StudyID = as.factor(meta_data$StudyID)
meta_data$StudyID = as.character(meta_data$StudyID)

#brm = glmer(outcome ~ the effect of an intercept)
Model = brm(MeanES|se(SdES) ~ 1 + (1|StudyID), data = meta_data, cores = 2, iter = 2000, chain = 2)
summary(Model)

#cores = the models take time to run, thereofore we split them on different processors 
#chain
#iter = how long time to look (I think)
#priors = for this, we can use the default priors 



forest(Model, show_data = TRUE, av_name = "Effect size")
#fejl: Error in .f(.x[[i]], ...) : object 'Martínez-Sánchez et al. (2015)' not found

```


Step 2: Prepare the pitch SD data from last year
- the data is available as Assignment4PitchData.csv (thanks Celine)
- We do not know how to build random effects, yet. So we need to simplify the dataset to avoid interdependence between datapoint: How?  - we want only one data point pr. subject 
- Also, let's standardize the data, so that they are compatible with our meta-analytic prior (Cohen's d is measured in SDs) - our analysis should follow the same scale. scale the data!

```{r}

#get an overall sd from the pitch on all the trials 
overall_PitchSd = aggregate(pitch_data[, 7], list(pitch_data$ID_unique), mean)
#change the column names 
setnames(overall_PitchSd, "Group.1", "ID_unique")
setnames(overall_PitchSd, "PitchSD", "Overall_PitchSD")

#merge with full data set
Pitch_data_new = merge(overall_PitchSd, pitch_data, by = "ID_unique")

#use only necessary columns
final_pitch_data = data.frame(Pitch_data_new$ID_unique, Pitch_data_new$Overall_PitchSD, Pitch_data_new$diagnosis)

#remove duplicates
final_pitch_data = final_pitch_data[!duplicated(final_pitch_data), ]

#setnames 
setnames(final_pitch_data, "Pitch_data_new.ID_unique", "ID_unique")
setnames(final_pitch_data, "Pitch_data_new.Overall_PitchSD", "Overall_PitchSD")
setnames(final_pitch_data, "Pitch_data_new.diagnosis", "diagnosis")

#_____________________________________________



#standardize pitch sd
final_pitch_data$Overall_PitchSD.s = (final_pitch_data$Overall_PitchSD - mean(final_pitch_data$Overall_PitchSD))/sd(final_pitch_data$Overall_PitchSD)


```


Step 3: Build a regression model predicting Pitch SD from Diagnosis.
- how is the outcome distributed? (likelihood function)
- how are the parameters of the likelihood distribution distributed? Which predictors should they be conditioned on?
- use a skeptical/conservative prior for the effects of diagnosis. Remember you'll need to motivate it.

Motivation: 
beta: (0,.2) #between -.6/.6 (because 3 sd's on each side covers 99% of all the distribution - multiply .2 with the 3 sd's)
most biological phenomena are normally distributed so in case you do not have more information about a biological phenomena it makes sense to assume a Gaussian distribution 
we choose to assume sigma as a half cauchy distribution. Variation can be unfinitly big but the bigger it gets the more unlikely it most likely is. 
- Describe and plot the estimates. Evaluate model quality

```{r}

#Model2 = brm(Overall_PitchSD.s ~ diagnosis + (1|ID), data = Pitch_data_new, cores = 2, iter = 2000, chain = 2)
#summary(Model2)

Model2 <- map(
    alist(
        Overall_PitchSD.s ~ dnorm( mu , sigma ) , #pitch sd outcome = normally distibuted
        mu <- a + b*diagnosis ,
        a ~ dnorm(0,1), #normally distributed
        b ~ dnorm(0,0.1),  #norammly distributed - conservative prior
        sigma ~ dcauchy(0,2) #a cauchy distribution 
    ) ,
    data=final_pitch_data )
precis(Model2)


precis_m2 = plot(precis(Model2))




#how is it distributed? - we expect to be distributed normally 
#a lot of natural processes give rise to gaussian distributions 
#PitchSd ~ Normal (mu, sigma) 
#mu = alpha (mean value for contrls) + beta* diagnosis(mean diffence bestween controls and skizo)

#add priors:
#alpha ~ Normal(0,1)   #mean 0 because it is standardized. 1 in sd will leave it free to vary as much as the original data
#beta ~ Normal(0,1) #conservative 
#sigma ~ cauchy(0,2) #cauchy = going all the way to infinite

?sim

#simulate data from model
sim.pitch = sim(Model2, data = final_pitch_data, n = 1000)

#using bayesplot
pp_check_model2 = pp_check(final_pitch_data$Overall_PitchSD.s, sim.pitch, ppc_dens_overlay)
pp_check_model2

#poterior predictive plot on top of raw data
dens(sim.pitch, col = "red", xlim = c(-5, 5), ylim = c(0,1),  xlab = "PitchSD")
par(new=TRUE)
dens(final_pitch_data$Overall_PitchSD.s, xlim = c(-5, 5), ylim = c(0,1), xlab = "PitchSD")
title("PitchSD predicted ~ diagnosis")





```


Step 4: Now re-run the model with the meta-analytic prior
- Describe and plot the estimates. Evaluate model quality


data from the meta analytic prior: 
changing beta priors 
intercept
Se(int)
Sd(int)
Se(sd(int))

don't change priors for alpha (no information on mean pitch for control (intercept) from metaanalysis)
don't change for sigma (the sigma is for variance between the subjects and the meta is variance between studies)

```{r}

Model3 <- map(
    alist(
        Overall_PitchSD.s ~ dnorm( mu , sigma ) , #pitch sd outcome = normally distibuted
        mu <- a + b*diagnosis ,
        a ~ dnorm(0,1), #normally distributed
        b ~ dnorm(-0.6,0.32),  #norammly distributed metaanalytic prior (from the ealier exercise (intercept estimate))
        sigma ~ dcauchy(0,2) #a cauchy distribution 
    ) ,
    data=final_pitch_data )
precis(Model3)



#plot estimates
precis_m3 = plot(precis(Model3))

#simulate
sim.pitch.model3 = sim(Model3, data = final_pitch_data, n = 1000)

pp_check_model3 = pp_check(final_pitch_data$Overall_PitchSD.s, sim.pitch.model3, ppc_dens_overlay)
pp_check_model3

#posterior predictive plot
dens(sim.pitch.model3, col = "red", xlim = c(-5, 5), ylim = c(0,1),  xlab = "PitchSD")
par(new=TRUE)
dens(final_pitch_data$Overall_PitchSD.s, xlim = c(-5, 5), ylim = c(0,1), xlab = "PitchSD")
title("PitchSD predicted ~ diagnosis")




```


Step 5: Compare the models
- Plot priors and posteriors of the diagnosis effect in both models
- Compare posteriors between the two models
- Compare their relative distance from truth (WAIC)
- Discuss how they compare and whether any of them is best.

```{r}

#plot priors and posteriors
x <- seq(-3,3, length=1e5)
y <- dnorm(x, 0, 0.5) 
y.s <- dnorm(x, 0, 0.1) #sceptical
y.m <- dnorm(x, -0.60, 0.33) #meta
#prior_df = data.frame(x = rep(x,2), y = c(y, y.s, y.m), prior = c(rep("sceptical", length(y.s)), rep("meta", length(y.m)))
                                                                
                                                                  
ggplot(prior_df, aes(x = x, y = y, color = prior)) + geom_line()
#Beautiful
  #As we expected: sceptical is more narrow
  #Meta has a lower mean than original
  #All in all - beautiful priors and pretty plot




#compare 
plot_grid(pp_check_model2, pp_check_model3, labels = c('Model2_Conservative', 'Model3_Metaanalytic'))

plot_grid(precis_m2, precis_m3, labels = c("Model2_Conservative", "Model3_Metaanalytic"), ncol = 1, align = 'v')

plot(coeftab(Model2,Model3))

WAIC(Model2)
WAIC(Model3)

compare(Model2,Model3)



#compaing pp checks = looks very similar. looking at the estimates the estimates look different though but the effect might be compensated by moving the intercept 
#doesn't look like one model is better than the other 
#WAIC test put a very tiny advantage to model with meta analytic priors but the se being sd being sp big that this is not difference that matter. If we have to choose between the two models, we could use a conceptual argument to use the model with meta analytic priors. We have a lot of extra knowledge from previous studies and it makes sence to include this knowledge.  



#plotting posteriors 
#conservative priors
post <- extract.samples(Model2)

color_scheme_set("mix-teal-pink")

mcmc_hist(post,
transformations = list(), facet_args = list(), binwidth = 0.03,
freq = TRUE)

#metaanalytic
post_meta <- extract.samples(Model3)
color_scheme_set("mix-teal-orange")

mcmc_hist(post_meta,
transformations = list(), facet_args = list(), binwidth = 0.03,
freq = TRUE)

#dens conservative
color_scheme_set("pink")
mcmc_dens(post)

#dens meta
color_scheme_set("viridis")
mcmc_dens(post_meta)



```

```{r}
#try with map2stan()

Model4 <- map2stan(
    alist(
        Overall_PitchSD.s ~ dnorm( mu , sigma ) , #pitch sd outcome = normally distibuted
        mu <- a + b*diagnosis ,
        a ~ dnorm(0,1), 
        b ~ dnorm(0,0.1),  
        sigma ~ dcauchy(0,2)
    ) ,
    data=final_pitch_data,
    start=list(alpha=0,sigma=1) ,
    chains=2 , iter=4000 , warmup=1000)

Model5 <- map2stan(
    alist(
        Overall_PitchSD.s ~ dnorm( mu , sigma ) , #pitch sd outcome = normally distibuted
        mu <- a + b*diagnosis ,
        a ~ dnorm(0,1), 
        b ~ dnorm(-0.6,0.32),  
        sigma ~ dcauchy(0,2)
    ) ,
    data=final_pitch_data,
    start=list(alpha=0,sigma=1) ,
    chains=2 , iter=4000 , warmup=1000)

#simulate
sim.pitch.model4 = sim(Model4, data = final_pitch_data, n = 1000)
sim.pitch.model5 = sim(Model5, data = final_pitch_data, n = 1000)

pp_check_model4 = pp_check(final_pitch_data$Overall_PitchSD.s, sim.pitch.model4, ppc_dens_overlay)
pp_check_model5 = pp_check(final_pitch_data$Overall_PitchSD.s, sim.pitch.model5, ppc_dens_overlay)

plot_grid(pp_check_model4, pp_check_model5, labels = c('Model2_Conservative', 'Model3_Metaanalytic'))


```


Step 6: Prepare a nice write up of the analysis and answer the questions at the top.

Optional step 7: how skeptical should a prior be?
- Try different levels of skepticism and compare them using WAIC.

Optional step 8: Include other predictors
- Do age, gender and education improve the model?
- Should they be main effects or interactions?

Optional step 9: Bromance magic.
- explore the bromance code below including random effects (by default with weakly informative priors)
- learn how to change the prior
- explore effects of trial, age, gender, including the appropriate random slopes
- compare the models you created using WAIC and posterior predictive check (pp_check())


```{r}

brm_out <- brm(PitchSD ~ 1 + Diagnosis  +(1|ID_unique/Study), # Outcome as a function of the predictors as in lme4. 
               data=Data, # Define the data
               family=gaussian(), # Define the family. 
               iter = 5000, warmup = 2000, cores = 4)
summary(brm_out1)
plot(brm_out1)

```

